{"config": {"indexing": "full", "lang": ["en"], "min_search_length": 3, "prebuild_index": false, "separator": "[\\s\\-]+"}, "docs": [{"location": "", "text": "Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.", "title": "Home"}, {"location": "#welcome-to-mkdocs", "text": "For full documentation visit mkdocs.org .", "title": "Welcome to MkDocs"}, {"location": "#commands", "text": "mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.", "title": "Commands"}, {"location": "#project-layout", "text": "mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.", "title": "Project layout"}, {"location": "getting-started/", "text": "Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.", "title": "Getting Started"}, {"location": "getting-started/#welcome-to-mkdocs", "text": "For full documentation visit mkdocs.org .", "title": "Welcome to MkDocs"}, {"location": "getting-started/#commands", "text": "mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.", "title": "Commands"}, {"location": "getting-started/#project-layout", "text": "mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.", "title": "Project layout"}, {"location": "references/", "text": "Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.", "title": "Welcome to MkDocs"}, {"location": "references/#welcome-to-mkdocs", "text": "For full documentation visit mkdocs.org .", "title": "Welcome to MkDocs"}, {"location": "references/#commands", "text": "mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.", "title": "Commands"}, {"location": "references/#project-layout", "text": "mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.", "title": "Project layout"}, {"location": "kitodo-production-docker/", "text": "Kitodo.Production Docker Prerequisites Builder Resource Builder Image Builder Usage Single compose project Multi compose project With the docker image provided, Kitodo.Production can be started in no time at all. A MySQL/MariaDB database and ElasticSearch must be present to start the application. There is also a docker-compose file for a quick start. Prerequisites Install Docker Engine https://docs.docker.com/get-docker/ Install Docker Compose https://docs.docker.com/compose/install/ Go to the directory where you've put docker-compose.yml. Builder Resource Builder The resource builder use a git release tag or git repository archive as source to generate build resources. These are provided to the image builder via the build argument BUILD_RESOURCES . Types First you have to decide which type to use for providing the build resources Release (default) Release files will be downloaded, renamed and moved to build resource folder. Git Archive with specified commit / branch and source url will be downloaded. Next builder triggers maven to build sources, creates database and migrate database using flyway migration steps. After build resource files will be renamed and moved to build resource folder. Environment variables Name Default Description APP_BUILD_CONTEXT . Directory of Dockerfile-Builder APP_BUILD_RESOURCES build-resources Directory of build resource results for application BUILDER_TYPE RELEASE available types RELEASE and GIT - RELEASE means build the build resources by a Kitodo.Production Release and its assets - GIT means build the build resources by commit/branch and BUILDER_RELEASE_VERSION 3.4.3 Release version name BUILDER_RELEASE_WAR_NAME kitodo-3.4.3 Release asset war file name BUILDER_RELEASE_SQL_NAME kitodo_3-4-3 Release assets sql file name BUILDER_RELEASE_CONFIG_MODULES_NAME kitodo_3-4-3_config_modules Release asset config modules zip file name BUILDER_GIT_COMMIT master Branch or commit of BUILDER_GIT_SOURCE_URL BUILDER_GIT_SOURCE_URL https://github.com/kitodo/kitodo-production/ Repository of BUILDER_GIT_COMMIT DB_PORT 3306 Port of database DB_ROOT_PASSWORD 1234 Root password DB_NAME kitodo Database name of Kitodo.Production DB_USER kitodo User of DB_NAME DB_USER_PASSWORD kitodo Password of DB_USER Usage with docker-compose Start building resources docker-compose -f ./docker-compose.yml -f ./docker-compose-builder.yml up --build kitodo-builder Remove resource builder docker-compose -f ./docker-compose.yml -f ./docker-compose-builder.yml down Image Builder The image contains the WAR, the database file and the config modules of the corresponding release for the Docker image tag. docker pull markusweigelt/kitodo-production:TAG After the container has been started Kitodo.Production can be reached at http://localhost:8080/kitodo with initial credentials username \"testadmin\" and password \"test\". Build arguments Name Default Description BUILD_RESOURCES Directory of build resources BUILD_RESOURCE_WAR kitodo.war Name of application war in build resource directory BUILD_RESOURCE_CONFIG_MODULES kitodo-config-modules.zip Name of config modules zip in build resource directory BUILD_RESOURCE_SQL kitodo.sql Name of sql dump in build resource directory Environment variables Name Default Description KITODO_DB_HOST localhost Host of MySQL or MariaDB database KITODO_DB_PORT 3306 Port of MySQL or MariaDB database KITODO_DB_NAME kitodo Name of database used by Kitodo.Productions KITODO_DB_USER kitodo Username to access database KITODO_DB_PASSWORD kitodo Password used by username to access database KITODO_ES_HOST localhost Host of Elasticsearch KITODO_MQ_HOST localhost Host of Active MQ KITODO_MQ_PORT 61616 Port of Active MQ Targets Name Path Description Config Modules /usr/local/kitodo If the directory is mounted or bind per volume and is empty, then it will be prefilled with the provided config modules of the release. Database If the database is still empty, it will be initialized with the database script from the release. Usage There are the following two options of usage. Single compose project If only one project instance is needed or repository is used as submodule in other projects. Build image before and start the container of image docker-compose up -d --build Stops the container docker-compose stop Stops and remove the container docker-compose down Multi compose project Go to the directory where you've put docker-compose.yml. Create subdirectory where you want to store your compose projects. In our examples we named it \"projects\". Create project directory (e.g. my-compose-project) in subdirectory where you want to store your compose project data. Usage with single env file Copy .env.example , rename file to .env , uncomment ${COMPOSE_PROJECT_NAME} and change value APP_BUILD_CONTEXT to ./projects/${COMPOSE_PROJECT_NAME} . docker-compose -p my-compose-project ... # ... means command e.g. up -d --build Usage with seperate env file Copy the .env.example to project directory, rename file to .env and change value of COMPOSE_PROJECT_NAME env to the name of project directory and APP_BUILD_CONTEXT to ./projects/${COMPOSE_PROJECT_NAME} . docker-compose --env-file ./projects/my-compose-project/.env ... # ... means command e.g. up -d --build", "title": "README"}, {"location": "kitodo-production-docker/#kitodoproduction-docker", "text": "Prerequisites Builder Resource Builder Image Builder Usage Single compose project Multi compose project With the docker image provided, Kitodo.Production can be started in no time at all. A MySQL/MariaDB database and ElasticSearch must be present to start the application. There is also a docker-compose file for a quick start.", "title": "Kitodo.Production Docker"}, {"location": "kitodo-production-docker/#prerequisites", "text": "Install Docker Engine https://docs.docker.com/get-docker/ Install Docker Compose https://docs.docker.com/compose/install/ Go to the directory where you've put docker-compose.yml.", "title": "Prerequisites"}, {"location": "kitodo-production-docker/#builder", "text": "", "title": "Builder"}, {"location": "kitodo-production-docker/#resource-builder", "text": "The resource builder use a git release tag or git repository archive as source to generate build resources. These are provided to the image builder via the build argument BUILD_RESOURCES .", "title": "Resource Builder"}, {"location": "kitodo-production-docker/#types", "text": "First you have to decide which type to use for providing the build resources", "title": "Types"}, {"location": "kitodo-production-docker/#release-default", "text": "Release files will be downloaded, renamed and moved to build resource folder.", "title": "Release (default)"}, {"location": "kitodo-production-docker/#git", "text": "Archive with specified commit / branch and source url will be downloaded. Next builder triggers maven to build sources, creates database and migrate database using flyway migration steps. After build resource files will be renamed and moved to build resource folder.", "title": "Git"}, {"location": "kitodo-production-docker/#environment-variables", "text": "Name Default Description APP_BUILD_CONTEXT . Directory of Dockerfile-Builder APP_BUILD_RESOURCES build-resources Directory of build resource results for application BUILDER_TYPE RELEASE available types RELEASE and GIT - RELEASE means build the build resources by a Kitodo.Production Release and its assets - GIT means build the build resources by commit/branch and BUILDER_RELEASE_VERSION 3.4.3 Release version name BUILDER_RELEASE_WAR_NAME kitodo-3.4.3 Release asset war file name BUILDER_RELEASE_SQL_NAME kitodo_3-4-3 Release assets sql file name BUILDER_RELEASE_CONFIG_MODULES_NAME kitodo_3-4-3_config_modules Release asset config modules zip file name BUILDER_GIT_COMMIT master Branch or commit of BUILDER_GIT_SOURCE_URL BUILDER_GIT_SOURCE_URL https://github.com/kitodo/kitodo-production/ Repository of BUILDER_GIT_COMMIT DB_PORT 3306 Port of database DB_ROOT_PASSWORD 1234 Root password DB_NAME kitodo Database name of Kitodo.Production DB_USER kitodo User of DB_NAME DB_USER_PASSWORD kitodo Password of DB_USER", "title": "Environment variables"}, {"location": "kitodo-production-docker/#usage-with-docker-compose", "text": "Start building resources docker-compose -f ./docker-compose.yml -f ./docker-compose-builder.yml up --build kitodo-builder Remove resource builder docker-compose -f ./docker-compose.yml -f ./docker-compose-builder.yml down", "title": "Usage with docker-compose"}, {"location": "kitodo-production-docker/#image-builder", "text": "The image contains the WAR, the database file and the config modules of the corresponding release for the Docker image tag. docker pull markusweigelt/kitodo-production:TAG After the container has been started Kitodo.Production can be reached at http://localhost:8080/kitodo with initial credentials username \"testadmin\" and password \"test\".", "title": "Image Builder"}, {"location": "kitodo-production-docker/#build-arguments", "text": "Name Default Description BUILD_RESOURCES Directory of build resources BUILD_RESOURCE_WAR kitodo.war Name of application war in build resource directory BUILD_RESOURCE_CONFIG_MODULES kitodo-config-modules.zip Name of config modules zip in build resource directory BUILD_RESOURCE_SQL kitodo.sql Name of sql dump in build resource directory", "title": "Build arguments"}, {"location": "kitodo-production-docker/#environment-variables_1", "text": "Name Default Description KITODO_DB_HOST localhost Host of MySQL or MariaDB database KITODO_DB_PORT 3306 Port of MySQL or MariaDB database KITODO_DB_NAME kitodo Name of database used by Kitodo.Productions KITODO_DB_USER kitodo Username to access database KITODO_DB_PASSWORD kitodo Password used by username to access database KITODO_ES_HOST localhost Host of Elasticsearch KITODO_MQ_HOST localhost Host of Active MQ KITODO_MQ_PORT 61616 Port of Active MQ", "title": "Environment variables"}, {"location": "kitodo-production-docker/#targets", "text": "Name Path Description Config Modules /usr/local/kitodo If the directory is mounted or bind per volume and is empty, then it will be prefilled with the provided config modules of the release.", "title": "Targets"}, {"location": "kitodo-production-docker/#database", "text": "If the database is still empty, it will be initialized with the database script from the release.", "title": "Database"}, {"location": "kitodo-production-docker/#usage", "text": "There are the following two options of usage.", "title": "Usage"}, {"location": "kitodo-production-docker/#single-compose-project", "text": "If only one project instance is needed or repository is used as submodule in other projects. Build image before and start the container of image docker-compose up -d --build Stops the container docker-compose stop Stops and remove the container docker-compose down", "title": "Single compose project"}, {"location": "kitodo-production-docker/#multi-compose-project", "text": "Go to the directory where you've put docker-compose.yml. Create subdirectory where you want to store your compose projects. In our examples we named it \"projects\". Create project directory (e.g. my-compose-project) in subdirectory where you want to store your compose project data.", "title": "Multi compose project"}, {"location": "kitodo-production-docker/#usage-with-single-env-file", "text": "Copy .env.example , rename file to .env , uncomment ${COMPOSE_PROJECT_NAME} and change value APP_BUILD_CONTEXT to ./projects/${COMPOSE_PROJECT_NAME} . docker-compose -p my-compose-project ... # ... means command e.g. up -d --build", "title": "Usage with single env file"}, {"location": "kitodo-production-docker/#usage-with-seperate-env-file", "text": "Copy the .env.example to project directory, rename file to .env and change value of COMPOSE_PROJECT_NAME env to the name of project directory and APP_BUILD_CONTEXT to ./projects/${COMPOSE_PROJECT_NAME} . docker-compose --env-file ./projects/my-compose-project/.env ... # ... means command e.g. up -d --build", "title": "Usage with seperate env file"}, {"location": "ocr-d-controller/", "text": "OCR-D Controller Path to network implementation of OCR-D In the simplest (and current) form, the controller will be a SSH login server for a full command-line OCR-D installation . Files must be mounted locally (if they are network shares, this must be done on the host side running the container). Next, the SSH server can also dynamically receive and send data. The first true network implementation will offer an HTTP interface for processing (like the workflow server ). From there, the actual processing could be further delegated into different processing servers. A more powerful workflow engine would then offer instantiating different workflows, and monitoring jobs. In the final form, the controller will implement (most parts of) the OCR-D Web API. Usage Building Starting and mounting General management Processing Workflow server Data transfer Parallel options Logging See also Usage Building Build or pull the Docker image: make build # or docker pull bertsky/ocrd_controller Starting and mounting Then run the container \u2013 providing host-side directories for the volumes \u2026 DATA : directory for data processing (including images or existing workspaces), defaults to current working directory MODELS : directory for persistent storage of processor resource files , defaults to ~/.local/share ; models will be under ./ocrd-resources/* CONFIG : directory for persistent storage of processor resource list , defaults to ~/.config ; file will be under ./ocrd/resources.yml \u2026 but also a file KEYS with public key credentials for log-in to the controller, and (optionally) some environment variables \u2026 WORKERS : number of parallel jobs (i.e. concurrent login sessions for ocrd ) (should be set to match the available computing resources) UID : numerical user identifier to be used by programs in the container (will affect the files modified/created); defaults to current user GID : numerical group identifier to be used by programs in the container (will affect the files modified/created); defaults to current group UMASK : numerical user mask to be used by programs in the container (will affect the files modified/created); defaults to 0002 PORT : numerical TCP port to expose the SSH server on the host side defaults to 8022 (for non-priviledged access) NETWORK name of the Docker network to use defaults to bridge (the default Docker network) \u2026 thus, for example : make run DATA=/mnt/workspaces MODELS=~/.local/share KEYS=~/.ssh/id_rsa.pub PORT=8022 WORKERS=3 General management Then you can log in as user ocrd from remote (but let's use controller in the following \u2013 without loss of generality): ssh -p 8022 ocrd@controller bash -i Unless you already have the data in workspaces , you need to create workspaces prior to processing. For example: ssh -p 8022 ocrd@controller \"ocrd-import -P some-document\" For actual processing, you will first need to download some models into your MODELS volume: ssh -p 8022 ocrd@controller \"ocrd resmgr download ocrd-tesserocr-recognize *\" Processing Subsequently, you can use these models on your DATA files: ssh -p 8022 ocrd@controller \"ocrd process -m some-document/mets.xml 'tesserocr-recognize -P segmentation_level region -P model Fraktur'\" # or equivalently: ssh -p 8022 ocrd@controller \"ocrd-tesserocr-recognize -m some-document/mets.xml -P segmentation_level region -P model Fraktur\" Workflow server Currently, the OCR-D installation hosts an implementation of the workflow server , which can be used to significantly reduce initialization overhead when running the same workflow repeatedly on many workspaces (especially with GPU-bound processors): ssh -p 8022 ocrd@controller \"ocrd workflow server -j 4 -t 120 'tesserocr-recognize -P segmentation_level region -P model Fraktur'\" And subsequently: ssh -p 8022 ocrd@controller \"ocrd workflow client process -m some-document/mets.xml\" ssh -p 8022 ocrd@controller \"ocrd workflow client process -m other-document/mets.xml\" Data transfer If your data files cannot be directly mounted on the host (not even as a network share), then you can use rsync , scp or sftp to transfer them to the server: rsync --port 8022 -av some-directory ocrd@controller:/data scp -P 8022 -r some-directory ocrd@controller:/data echo put some-directory /data | sftp -P 8022 ocrd@controller Analogously, to transfer the results back: rsync --port 8022 -av ocrd@controller:/data/some-directory . scp -P 8022 -r ocrd@controller:/data/some-directory . echo get /data/some-directory | sftp -P 8022 ocrd@controller Parallel options For parallel processing, you can either - run multiple processes on a single controller by - logging in multiple times, or - issueing parallel commands \u2013 * via basic shell scripting * via ocrd-make calls * via ocrd workflow server --processes concurrency - run processes on multiple controllers. Note: internally, WORKERS is implemented as a (GNU parallel-based) semaphore wrapping the SSH sessions inside blocking sem --fg calls within .ssh/rc. Thus, commands will get queued but not processed until a 'worker' is free. Logging All logs are accumulated on standard output, which can be inspected via Docker: docker logs ocrd_controller See also Meta-repo for integration of Kitodo.Production with OCR-D in Docker Sister component OCR-D Manager", "title": "README"}, {"location": "ocr-d-controller/#ocr-d-controller", "text": "Path to network implementation of OCR-D In the simplest (and current) form, the controller will be a SSH login server for a full command-line OCR-D installation . Files must be mounted locally (if they are network shares, this must be done on the host side running the container). Next, the SSH server can also dynamically receive and send data. The first true network implementation will offer an HTTP interface for processing (like the workflow server ). From there, the actual processing could be further delegated into different processing servers. A more powerful workflow engine would then offer instantiating different workflows, and monitoring jobs. In the final form, the controller will implement (most parts of) the OCR-D Web API. Usage Building Starting and mounting General management Processing Workflow server Data transfer Parallel options Logging See also", "title": "OCR-D Controller"}, {"location": "ocr-d-controller/#usage", "text": "", "title": "Usage"}, {"location": "ocr-d-controller/#building", "text": "Build or pull the Docker image: make build # or docker pull bertsky/ocrd_controller", "title": "Building"}, {"location": "ocr-d-controller/#starting-and-mounting", "text": "Then run the container \u2013 providing host-side directories for the volumes \u2026 DATA : directory for data processing (including images or existing workspaces), defaults to current working directory MODELS : directory for persistent storage of processor resource files , defaults to ~/.local/share ; models will be under ./ocrd-resources/* CONFIG : directory for persistent storage of processor resource list , defaults to ~/.config ; file will be under ./ocrd/resources.yml \u2026 but also a file KEYS with public key credentials for log-in to the controller, and (optionally) some environment variables \u2026 WORKERS : number of parallel jobs (i.e. concurrent login sessions for ocrd ) (should be set to match the available computing resources) UID : numerical user identifier to be used by programs in the container (will affect the files modified/created); defaults to current user GID : numerical group identifier to be used by programs in the container (will affect the files modified/created); defaults to current group UMASK : numerical user mask to be used by programs in the container (will affect the files modified/created); defaults to 0002 PORT : numerical TCP port to expose the SSH server on the host side defaults to 8022 (for non-priviledged access) NETWORK name of the Docker network to use defaults to bridge (the default Docker network) \u2026 thus, for example : make run DATA=/mnt/workspaces MODELS=~/.local/share KEYS=~/.ssh/id_rsa.pub PORT=8022 WORKERS=3", "title": "Starting and mounting"}, {"location": "ocr-d-controller/#general-management", "text": "Then you can log in as user ocrd from remote (but let's use controller in the following \u2013 without loss of generality): ssh -p 8022 ocrd@controller bash -i Unless you already have the data in workspaces , you need to create workspaces prior to processing. For example: ssh -p 8022 ocrd@controller \"ocrd-import -P some-document\" For actual processing, you will first need to download some models into your MODELS volume: ssh -p 8022 ocrd@controller \"ocrd resmgr download ocrd-tesserocr-recognize *\"", "title": "General management"}, {"location": "ocr-d-controller/#processing", "text": "Subsequently, you can use these models on your DATA files: ssh -p 8022 ocrd@controller \"ocrd process -m some-document/mets.xml 'tesserocr-recognize -P segmentation_level region -P model Fraktur'\" # or equivalently: ssh -p 8022 ocrd@controller \"ocrd-tesserocr-recognize -m some-document/mets.xml -P segmentation_level region -P model Fraktur\"", "title": "Processing"}, {"location": "ocr-d-controller/#workflow-server", "text": "Currently, the OCR-D installation hosts an implementation of the workflow server , which can be used to significantly reduce initialization overhead when running the same workflow repeatedly on many workspaces (especially with GPU-bound processors): ssh -p 8022 ocrd@controller \"ocrd workflow server -j 4 -t 120 'tesserocr-recognize -P segmentation_level region -P model Fraktur'\" And subsequently: ssh -p 8022 ocrd@controller \"ocrd workflow client process -m some-document/mets.xml\" ssh -p 8022 ocrd@controller \"ocrd workflow client process -m other-document/mets.xml\"", "title": "Workflow server"}, {"location": "ocr-d-controller/#data-transfer", "text": "If your data files cannot be directly mounted on the host (not even as a network share), then you can use rsync , scp or sftp to transfer them to the server: rsync --port 8022 -av some-directory ocrd@controller:/data scp -P 8022 -r some-directory ocrd@controller:/data echo put some-directory /data | sftp -P 8022 ocrd@controller Analogously, to transfer the results back: rsync --port 8022 -av ocrd@controller:/data/some-directory . scp -P 8022 -r ocrd@controller:/data/some-directory . echo get /data/some-directory | sftp -P 8022 ocrd@controller", "title": "Data transfer"}, {"location": "ocr-d-controller/#parallel-options", "text": "For parallel processing, you can either - run multiple processes on a single controller by - logging in multiple times, or - issueing parallel commands \u2013 * via basic shell scripting * via ocrd-make calls * via ocrd workflow server --processes concurrency - run processes on multiple controllers. Note: internally, WORKERS is implemented as a (GNU parallel-based) semaphore wrapping the SSH sessions inside blocking sem --fg calls within .ssh/rc. Thus, commands will get queued but not processed until a 'worker' is free.", "title": "Parallel options"}, {"location": "ocr-d-controller/#logging", "text": "All logs are accumulated on standard output, which can be inspected via Docker: docker logs ocrd_controller", "title": "Logging"}, {"location": "ocr-d-controller/#see-also", "text": "Meta-repo for integration of Kitodo.Production with OCR-D in Docker Sister component OCR-D Manager", "title": "See also"}, {"location": "ocr-d-manager/", "text": "OCR-D Manager OCR-D Manager is a server that mediates between Kitodo and OCR-D . It resides on the site of the Kitodo installation (so the actual OCR server can be managed independently) but runs in its own container (so Kitodo can be managed independently). Specifically, it gets called by Kitodo.Production or Kitodo.Presentation to handle OCR for a document, and in turn calls the OCR-D Controller for workflow processing. For an integration as a service container , orchestrated with other containers (Kitodo+Controller), see this meta-repo . OCR-D Manager is responsible for - data transfer from Kitodo to Controller and back, - delegation to Controller, - signalling/reporting, - result validation, - result extraction (putting ALTO files in the process directory where Kitodo expects them). It is currently implemented as SSH login server with an installation of OCR-D core and an SSH client to connect to the Controller. Usage Building Starting and mounting General management Processing Data transfer Logging Monitoring Testing Usage Building Build or pull the Docker image: make build # or docker pull markusweigelt/ocrd_manager Starting and mounting Then run the container \u2013 providing a host-side directory for the volume \u2026 DATA : directory for data processing (including images or existing workspaces), defaults to current working directory \u2026 but also files \u2026 KEYS : public key credentials for log-in to the manager PRIVATE : private key credentials for log-in to the controller \u2026 \u2026 and (optionally) some environment variables \u2026 UID : numerical user identifier to be used by programs in the container (will affect the files modified/created); defaults to current user GID : numerical group identifier to be used by programs in the container (will affect the files modified/created); defaults to current group UMASK : numerical user mask to be used by programs in the container (will affect the files modified/created); defaults to 0002 PORT : numerical TCP port to expose the SSH server on the host side defaults to 9022 (for non-priviledged access) CONTROLLER network address:port for the controller client (must be reachable from the container network) ACTIVEMQ network address:port of ActiveMQ server listening to result status (must be reachable from the container network) NETWORK name of the Docker network to use defaults to bridge (the default Docker network) \u2026 thus, for example : make run DATA=/mnt/workspaces MODELS=~/.local/share KEYS=~/.ssh/id_rsa.pub PORT=9022 PRIVATE=~/.ssh/id_rsa (You can also run the service via docker-compose manually \u2013 just cp .env.example .env and edit to your needs.) General management Then you can log in as user ocrd from remote (but let's use manager in the following \u2013 without loss of generality): ssh -p 9022 ocrd@manager bash -i (Typically though, you will run a non-interactive script, see next section.) Processing In the Manager, you can run shell scripts that do - data management and validation via ocrd CLIs - OCR processing by running workflows in the controller via ssh ocrd@ocrd_controller log-ins The data management will depend on which Kitodo context you want to integrate into (Production 2 / 3 or Presentation). For Kitodo.Production, there is a preconfigured script for_production.sh which takes the following arguments: 1. process ID 2. task ID 3. directory path 4. language 5. script 6. workflow name The last argument (6) is optional and defaults to the preconfigured script ocr.sh which contains a trivial workflow: - import of the images into a new OCR-D workspace - preprocessing, layout analysis and text recognition with a single Tesseract processor call - format conversion of the result from PAGE-XML to ALTO-XML It can be replaced with the (path) name of any workflow script mounted under /data . For example (assuming testdata is a directory with image files mounted under /data ): ssh -T -p 9022 ocrd@manager for_production.sh 1 3 testdata deu Fraktur ocr.sh Data transfer For sharing data between the Manager and Controller, it is recommended to transfer files explicitly (as this will make the costs more measurable and controllable). (This is currently implemented via rsync .) The data lifecycle should be: - on Controller: short-lived - on Manager: as long as process is active in Production (This is currently not managed.) Logging All logs are accumulated on standard output, which can be inspected via Docker: docker logs ocrd_manager Monitoring The repo also provides a web server featuring - (intermediate) results for all current document workspaces (via OCR-D Browser ) - :construction: log viewer - :construction: task viewer - :construction: workflow editor Build or pull the Docker image: make build-monitor # or docker pull bertsky/ocrd_monitor Then run the container \u2013 providing the same variables as above: make run-monitor DATA=/mnt/workspaces You can then open http://localhost:8080 in your browser. Testing After building and starting , you can use the test target for a round-trip: make test DATA=/mnt/workspaces This will download sample data and run the default workflow on them.", "title": "README"}, {"location": "ocr-d-manager/#ocr-d-manager", "text": "OCR-D Manager is a server that mediates between Kitodo and OCR-D . It resides on the site of the Kitodo installation (so the actual OCR server can be managed independently) but runs in its own container (so Kitodo can be managed independently). Specifically, it gets called by Kitodo.Production or Kitodo.Presentation to handle OCR for a document, and in turn calls the OCR-D Controller for workflow processing. For an integration as a service container , orchestrated with other containers (Kitodo+Controller), see this meta-repo . OCR-D Manager is responsible for - data transfer from Kitodo to Controller and back, - delegation to Controller, - signalling/reporting, - result validation, - result extraction (putting ALTO files in the process directory where Kitodo expects them). It is currently implemented as SSH login server with an installation of OCR-D core and an SSH client to connect to the Controller. Usage Building Starting and mounting General management Processing Data transfer Logging Monitoring Testing", "title": "OCR-D Manager"}, {"location": "ocr-d-manager/#usage", "text": "", "title": "Usage"}, {"location": "ocr-d-manager/#building", "text": "Build or pull the Docker image: make build # or docker pull markusweigelt/ocrd_manager", "title": "Building"}, {"location": "ocr-d-manager/#starting-and-mounting", "text": "Then run the container \u2013 providing a host-side directory for the volume \u2026 DATA : directory for data processing (including images or existing workspaces), defaults to current working directory \u2026 but also files \u2026 KEYS : public key credentials for log-in to the manager PRIVATE : private key credentials for log-in to the controller \u2026 \u2026 and (optionally) some environment variables \u2026 UID : numerical user identifier to be used by programs in the container (will affect the files modified/created); defaults to current user GID : numerical group identifier to be used by programs in the container (will affect the files modified/created); defaults to current group UMASK : numerical user mask to be used by programs in the container (will affect the files modified/created); defaults to 0002 PORT : numerical TCP port to expose the SSH server on the host side defaults to 9022 (for non-priviledged access) CONTROLLER network address:port for the controller client (must be reachable from the container network) ACTIVEMQ network address:port of ActiveMQ server listening to result status (must be reachable from the container network) NETWORK name of the Docker network to use defaults to bridge (the default Docker network) \u2026 thus, for example : make run DATA=/mnt/workspaces MODELS=~/.local/share KEYS=~/.ssh/id_rsa.pub PORT=9022 PRIVATE=~/.ssh/id_rsa (You can also run the service via docker-compose manually \u2013 just cp .env.example .env and edit to your needs.)", "title": "Starting and mounting"}, {"location": "ocr-d-manager/#general-management", "text": "Then you can log in as user ocrd from remote (but let's use manager in the following \u2013 without loss of generality): ssh -p 9022 ocrd@manager bash -i (Typically though, you will run a non-interactive script, see next section.)", "title": "General management"}, {"location": "ocr-d-manager/#processing", "text": "In the Manager, you can run shell scripts that do - data management and validation via ocrd CLIs - OCR processing by running workflows in the controller via ssh ocrd@ocrd_controller log-ins The data management will depend on which Kitodo context you want to integrate into (Production 2 / 3 or Presentation). For Kitodo.Production, there is a preconfigured script for_production.sh which takes the following arguments: 1. process ID 2. task ID 3. directory path 4. language 5. script 6. workflow name The last argument (6) is optional and defaults to the preconfigured script ocr.sh which contains a trivial workflow: - import of the images into a new OCR-D workspace - preprocessing, layout analysis and text recognition with a single Tesseract processor call - format conversion of the result from PAGE-XML to ALTO-XML It can be replaced with the (path) name of any workflow script mounted under /data . For example (assuming testdata is a directory with image files mounted under /data ): ssh -T -p 9022 ocrd@manager for_production.sh 1 3 testdata deu Fraktur ocr.sh", "title": "Processing"}, {"location": "ocr-d-manager/#data-transfer", "text": "For sharing data between the Manager and Controller, it is recommended to transfer files explicitly (as this will make the costs more measurable and controllable). (This is currently implemented via rsync .) The data lifecycle should be: - on Controller: short-lived - on Manager: as long as process is active in Production (This is currently not managed.)", "title": "Data transfer"}, {"location": "ocr-d-manager/#logging", "text": "All logs are accumulated on standard output, which can be inspected via Docker: docker logs ocrd_manager", "title": "Logging"}, {"location": "ocr-d-manager/#monitoring", "text": "The repo also provides a web server featuring - (intermediate) results for all current document workspaces (via OCR-D Browser ) - :construction: log viewer - :construction: task viewer - :construction: workflow editor Build or pull the Docker image: make build-monitor # or docker pull bertsky/ocrd_monitor Then run the container \u2013 providing the same variables as above: make run-monitor DATA=/mnt/workspaces You can then open http://localhost:8080 in your browser.", "title": "Monitoring"}, {"location": "ocr-d-manager/#testing", "text": "After building and starting , you can use the test target for a round-trip: make test DATA=/mnt/workspaces This will download sample data and run the default workflow on them.", "title": "Testing"}, {"location": "setup/configuration/", "text": "Configuration Note : By default this is done in .env file (with shell-like syntax). But any environment variable settings via shell or make call take precedence of .env configuration. So for example, you can source .env and then customize the default values interactively. Alternatively, you can import a customized file, e.g. source .env.local .) Note : When using the Makefile, some of the static settings in .env will always be overridden dynamically: To optimally match ownership and permissions of existing files with new data to be written, the UID and GID of the ocrd user on the Controller and Manager are taken from the host system. Note : If you do not want to make build the images yourself, but use the prebuilt images from Dockerhub, then mind the image tag variables. Instead of the default latest development version, you may want to use a stable release: Just checkout the respective Git release, and .env should already point to stable tags.", "title": "Overview"}, {"location": "setup/configuration/#configuration", "text": "Note : By default this is done in .env file (with shell-like syntax). But any environment variable settings via shell or make call take precedence of .env configuration. So for example, you can source .env and then customize the default values interactively. Alternatively, you can import a customized file, e.g. source .env.local .) Note : When using the Makefile, some of the static settings in .env will always be overridden dynamically: To optimally match ownership and permissions of existing files with new data to be written, the UID and GID of the ocrd user on the Controller and Manager are taken from the host system. Note : If you do not want to make build the images yourself, but use the prebuilt images from Dockerhub, then mind the image tag variables. Instead of the default latest development version, you may want to use a stable release: Just checkout the respective Git release, and .env should already point to stable tags.", "title": "Configuration"}, {"location": "setup/preparation/", "text": "Preparation Go to the directory where you've checked out the project. Prepare data structure Before Docker Compose can be used, you must create directories to mount SSH key pairs for user authentication to OCR-D Controller and OCR-D Manager . Moreover, for testing we need example data (e.g. users, authorities, workflows etc.) set up in the database of Kitodo.Production. Finally, you need to install some OCR models in the Controller with OCR-D Resource Manager. The fastest way to get all that is by using the Makefile via the following command: make prepare Note: This may not meet your exact scenario. To customize, have a look at the rules , or simulate running them via make -n prepare , or modify the results afterwards. (For example, if you have set up the OCR-D Controller externally , you will have to manually append to its authorized_keys the file generated under ./ocrd/manager/.ssh/id_rsa.pub , or copy the existing private key into ./ocrd/manager/.ssh/id_rsa .) Alternatively, perform the following steps manually: Create directories to host SSH key pair files: mkdir ./kitodo/.ssh/ mkdir ./ocrd/manager/.ssh/ mkdir ./ocrd/controller/.ssh/ Generate SSH key pairs in ./kitodo/.ssh/ and ./ocrd/manager/.ssh/ . After that, register the public keys on the other side, respectively: mv ./kitodo/.ssh/id_rsa.pub ./ocrd/manager/authorized_keys mv ./ocrd/manager/id_rsa.pub ./ocrd/controller/authorized_keys Unzip ./_resources/data.zip to ./_resources/data to provide the examples and Kitodo.Production configuration files. Follow the instructions in the next section to install OCR models on the OCR-D Controller. Install models on the OCR-D Controller For practical workflows, you first have to install models for various processors on the OCR-D Controller. Since all processor resources are mounted under the CONTROLLER_MODELS volume, resources will persist and thus only have to be installed once. Installation could be done by downloading the respective files into the filesystem (see make prepare ), or dynamically: Start interactive shell on the Controller. docker exec -it kitodo_production_ocrd_ocrd-controller_1 bash su - ocrd Use the OCR-D Resource Manager to query and install models: wget -O frak2021.traineddata https://ub-backup.bib.uni-mannheim.de/~stweil/tesstrain/frak2021/tessdata_best/frak2021-0.905.traineddata ocrd resmgr download -n ocrd-tesserocr-recognize frak2021.traineddata ocrd resmgr download ocrd-eynollah-segment default ocrd resmgr list-installed ocrd resmgr list-available ocrd resmgr --help Building Docker images Unless you want to run with prebuilt images from Dockerhub (in which case make sure you have configured the right version tags in your .env ), you first need to build Docker images for all modules. For the Kitodo.Production application , prior to building the app container itself, one needs to compile the app from source (the result of which will be provided in the submodule kitodo-production-docker under folder build-resources ). From there, all runtime module images can be built: make build (or equivalently:) docker-compose -f ./docker-compose.kitodo-builder.yml up --abort-on-container-exit --build docker-compose -f ./docker-compose.kitodo-builder.yml down docker-compose build", "title": "Preparation"}, {"location": "setup/preparation/#preparation", "text": "Go to the directory where you've checked out the project.", "title": "Preparation"}, {"location": "setup/preparation/#prepare-data-structure", "text": "Before Docker Compose can be used, you must create directories to mount SSH key pairs for user authentication to OCR-D Controller and OCR-D Manager . Moreover, for testing we need example data (e.g. users, authorities, workflows etc.) set up in the database of Kitodo.Production. Finally, you need to install some OCR models in the Controller with OCR-D Resource Manager. The fastest way to get all that is by using the Makefile via the following command: make prepare Note: This may not meet your exact scenario. To customize, have a look at the rules , or simulate running them via make -n prepare , or modify the results afterwards. (For example, if you have set up the OCR-D Controller externally , you will have to manually append to its authorized_keys the file generated under ./ocrd/manager/.ssh/id_rsa.pub , or copy the existing private key into ./ocrd/manager/.ssh/id_rsa .) Alternatively, perform the following steps manually: Create directories to host SSH key pair files: mkdir ./kitodo/.ssh/ mkdir ./ocrd/manager/.ssh/ mkdir ./ocrd/controller/.ssh/ Generate SSH key pairs in ./kitodo/.ssh/ and ./ocrd/manager/.ssh/ . After that, register the public keys on the other side, respectively: mv ./kitodo/.ssh/id_rsa.pub ./ocrd/manager/authorized_keys mv ./ocrd/manager/id_rsa.pub ./ocrd/controller/authorized_keys Unzip ./_resources/data.zip to ./_resources/data to provide the examples and Kitodo.Production configuration files. Follow the instructions in the next section to install OCR models on the OCR-D Controller.", "title": "Prepare data structure"}, {"location": "setup/preparation/#install-models-on-the-ocr-d-controller", "text": "For practical workflows, you first have to install models for various processors on the OCR-D Controller. Since all processor resources are mounted under the CONTROLLER_MODELS volume, resources will persist and thus only have to be installed once. Installation could be done by downloading the respective files into the filesystem (see make prepare ), or dynamically: Start interactive shell on the Controller. docker exec -it kitodo_production_ocrd_ocrd-controller_1 bash su - ocrd Use the OCR-D Resource Manager to query and install models: wget -O frak2021.traineddata https://ub-backup.bib.uni-mannheim.de/~stweil/tesstrain/frak2021/tessdata_best/frak2021-0.905.traineddata ocrd resmgr download -n ocrd-tesserocr-recognize frak2021.traineddata ocrd resmgr download ocrd-eynollah-segment default ocrd resmgr list-installed ocrd resmgr list-available ocrd resmgr --help", "title": "Install models on the OCR-D Controller"}, {"location": "setup/preparation/#building-docker-images", "text": "Unless you want to run with prebuilt images from Dockerhub (in which case make sure you have configured the right version tags in your .env ), you first need to build Docker images for all modules. For the Kitodo.Production application , prior to building the app container itself, one needs to compile the app from source (the result of which will be provided in the submodule kitodo-production-docker under folder build-resources ). From there, all runtime module images can be built: make build (or equivalently:) docker-compose -f ./docker-compose.kitodo-builder.yml up --abort-on-container-exit --build docker-compose -f ./docker-compose.kitodo-builder.yml down docker-compose build", "title": "Building Docker images"}, {"location": "setup/prerequisites/", "text": "Prerequisites Docker Install Docker Engine Install Docker Compose Install Nvidia Container Runtime (needed for Controller \u2013 even if no GPU is available) Git Either clone recursively in the first place: git clone --recurse-submodules https://github.com/markusweigelt/kitodo_production_ocrd cd kitodo_production_ocrd Or, after cloning and entering the repository normally, clone all submodules: git submodule update --init --recursive", "title": "Prerequisites"}, {"location": "setup/prerequisites/#prerequisites", "text": "", "title": "Prerequisites"}, {"location": "setup/prerequisites/#docker", "text": "Install Docker Engine Install Docker Compose Install Nvidia Container Runtime (needed for Controller \u2013 even if no GPU is available)", "title": "Docker"}, {"location": "setup/prerequisites/#git", "text": "Either clone recursively in the first place: git clone --recurse-submodules https://github.com/markusweigelt/kitodo_production_ocrd cd kitodo_production_ocrd Or, after cloning and entering the repository normally, clone all submodules: git submodule update --init --recursive", "title": "Git"}, {"location": "setup/configuration/environment-variables/", "text": "Environment Variables of Docker Compose The following variables must be defined. OCR-D Controller (only relevant in managed mode , see above ) Name Default Description CONTROLLER_IMAGE bertsky/ocrd_controller:latest name and tag of image CONTROLLER_HOST ocrd-controller name of host CONTROLLER_ENV_UID 1001 user id of SSH user ( id -u when using make ) CONTROLLER_ENV_GID 1001 group id of SSH user ( id -g when using make ) CONTROLLER_ENV_UMASK 0002 SSH user specific permission mask CONTROLLER_PORT_SSH 22 SSH port to reach CONTROLLER_KEYS ./ocrd/controller/.ssh/authorized_keys file with public SSH keys of users allowed to login from Manager or externally CONTROLLER_DATA ./kitodo/data/metadata data volume to mount CONTROLLER_MODELS ./ocrd/controller/models path to Controller models (in ocrd-resources/ ) CONTROLLER_CONFIG ./ocrd/controller/config path to Controller config (in ocrd/resources.yml ) CONTROLLER_WORKERS 1 count of workers for processing Manager Name Default Description MANAGER_IMAGE markusweigelt/ocrd_manager:latest name and tag of image MANAGER_HOST ocrd-manager name of host MANAGER_ENV_UID 1001 user id of SSH user ( id -u when using make ) MANAGER_ENV_GID 1001 group id of SSH user ( id -g when using make ) MANAGER_ENV_UMASK 0002 ssh user specific permission mask MANAGER_KEYS ./ocrd/manager/.ssh/authorized_keys file with public SSH keys of users allowed to login from Kitodo or externally MANAGER_KEY ./ocrd/manager/.ssh/id_rsa file with private SSH key of ocrd user for login to local ( managed ) or external Controller MANAGER_DATA ./kitodo/data/metadata data volume to mount (It is allowed and realistic if MANAGER_DATA is different than CONTROLLER_DATA . Input/output will be synchronized between them at runtime.) Monitor Name Default Description MONITOR_IMAGE bertsky/ocrd_monitor:latest name and tag of image MONITOR_HOST ocrd-monitor name of host MONITOR_PORT_WEB 5000 host-side port to exposed Web serve MONITOR_PORT_GTK 8085 host-side port to exposed Broadwayd (Gtk Web server) MONITOR_PORT_LOG 8088 host-side port to exposed Dozzle (Docker log viewer) MONITOR_DATA ./kitodo/data/metadata data volume to mount (Currently, MONITOR_DATA should be the same as MANAGER_DATA .) Kitodo.Production Resource Builder Name Default Description BUILDER_TYPE GIT Type of builder BUILDER_GIT_COMMIT ocrd-main branch \"ocrd-main\" of git repository BUILDER_GIT_SOURCE_URL https://github.com/markusweigelt/kitodo-production/ repository of BUILDER_GIT_COMMIT BUILDER_BUILD_RESOURCES ./_modules/kitodo-production-docker/kitodo/build-resources directory path to resources for building Kitodo.Production image Application Name Default Description APP_IMAGE markusweigelt/kitodo-production:latest name and tag of image APP_BUILD_CONTEXT ./_modules/kitodo-production-docker/kitodo/ directory of Dockerfile APP_BUILD_RESOURCES ./build-resources directory of build resources APP_DATA ./kitodo/data directory of application data e.g. config and modules APP_KEY ./kitodo/.ssh/id_rsa file with private ssh key of ocrd user to login to Manager APP_PORT 8080 port of Kitodo.Production Database Name Default Description DB_IMAGE mysql:8.0.26 name and tag of image DB_HOST kitodo-db host of database DB_PORT 3306 port of database DB_ROOT_PASSWORD 1234 root user password DB_NAME kitodo name of database used by Kitodo.Productions DB_USER kitodo username to access database DB_USER_PASSWORD kitodo password used by username to access database Elastic Search Name Default Description ES_IMAGE docker.elastic.co/elasticsearch/elasticsearch:7.17.3 name and tag of image ES_HOST kitodo-es host of elastic search ES_REST_PORT 9200 rest port ES_NODE_PORT 9300 node port Active MQ Name Default Description MQ_IMAGE markusweigelt/activemq:latest name and tag of image MQ_HOST kitodo-mq host of active mq MQ_PORT 61616 port of active mq", "title": "Environment Variables of Docker Compose"}, {"location": "setup/configuration/environment-variables/#environment-variables-of-docker-compose", "text": "The following variables must be defined.", "title": "Environment Variables of Docker Compose"}, {"location": "setup/configuration/environment-variables/#ocr-d", "text": "", "title": "OCR-D"}, {"location": "setup/configuration/environment-variables/#controller", "text": "(only relevant in managed mode , see above ) Name Default Description CONTROLLER_IMAGE bertsky/ocrd_controller:latest name and tag of image CONTROLLER_HOST ocrd-controller name of host CONTROLLER_ENV_UID 1001 user id of SSH user ( id -u when using make ) CONTROLLER_ENV_GID 1001 group id of SSH user ( id -g when using make ) CONTROLLER_ENV_UMASK 0002 SSH user specific permission mask CONTROLLER_PORT_SSH 22 SSH port to reach CONTROLLER_KEYS ./ocrd/controller/.ssh/authorized_keys file with public SSH keys of users allowed to login from Manager or externally CONTROLLER_DATA ./kitodo/data/metadata data volume to mount CONTROLLER_MODELS ./ocrd/controller/models path to Controller models (in ocrd-resources/ ) CONTROLLER_CONFIG ./ocrd/controller/config path to Controller config (in ocrd/resources.yml ) CONTROLLER_WORKERS 1 count of workers for processing", "title": "Controller"}, {"location": "setup/configuration/environment-variables/#manager", "text": "Name Default Description MANAGER_IMAGE markusweigelt/ocrd_manager:latest name and tag of image MANAGER_HOST ocrd-manager name of host MANAGER_ENV_UID 1001 user id of SSH user ( id -u when using make ) MANAGER_ENV_GID 1001 group id of SSH user ( id -g when using make ) MANAGER_ENV_UMASK 0002 ssh user specific permission mask MANAGER_KEYS ./ocrd/manager/.ssh/authorized_keys file with public SSH keys of users allowed to login from Kitodo or externally MANAGER_KEY ./ocrd/manager/.ssh/id_rsa file with private SSH key of ocrd user for login to local ( managed ) or external Controller MANAGER_DATA ./kitodo/data/metadata data volume to mount (It is allowed and realistic if MANAGER_DATA is different than CONTROLLER_DATA . Input/output will be synchronized between them at runtime.)", "title": "Manager"}, {"location": "setup/configuration/environment-variables/#monitor", "text": "Name Default Description MONITOR_IMAGE bertsky/ocrd_monitor:latest name and tag of image MONITOR_HOST ocrd-monitor name of host MONITOR_PORT_WEB 5000 host-side port to exposed Web serve MONITOR_PORT_GTK 8085 host-side port to exposed Broadwayd (Gtk Web server) MONITOR_PORT_LOG 8088 host-side port to exposed Dozzle (Docker log viewer) MONITOR_DATA ./kitodo/data/metadata data volume to mount (Currently, MONITOR_DATA should be the same as MANAGER_DATA .)", "title": "Monitor"}, {"location": "setup/configuration/environment-variables/#kitodoproduction", "text": "", "title": "Kitodo.Production"}, {"location": "setup/configuration/environment-variables/#resource-builder", "text": "Name Default Description BUILDER_TYPE GIT Type of builder BUILDER_GIT_COMMIT ocrd-main branch \"ocrd-main\" of git repository BUILDER_GIT_SOURCE_URL https://github.com/markusweigelt/kitodo-production/ repository of BUILDER_GIT_COMMIT BUILDER_BUILD_RESOURCES ./_modules/kitodo-production-docker/kitodo/build-resources directory path to resources for building Kitodo.Production image", "title": "Resource Builder"}, {"location": "setup/configuration/environment-variables/#application", "text": "Name Default Description APP_IMAGE markusweigelt/kitodo-production:latest name and tag of image APP_BUILD_CONTEXT ./_modules/kitodo-production-docker/kitodo/ directory of Dockerfile APP_BUILD_RESOURCES ./build-resources directory of build resources APP_DATA ./kitodo/data directory of application data e.g. config and modules APP_KEY ./kitodo/.ssh/id_rsa file with private ssh key of ocrd user to login to Manager APP_PORT 8080 port of Kitodo.Production", "title": "Application"}, {"location": "setup/configuration/environment-variables/#database", "text": "Name Default Description DB_IMAGE mysql:8.0.26 name and tag of image DB_HOST kitodo-db host of database DB_PORT 3306 port of database DB_ROOT_PASSWORD 1234 root user password DB_NAME kitodo name of database used by Kitodo.Productions DB_USER kitodo username to access database DB_USER_PASSWORD kitodo password used by username to access database", "title": "Database"}, {"location": "setup/configuration/environment-variables/#elastic-search", "text": "Name Default Description ES_IMAGE docker.elastic.co/elasticsearch/elasticsearch:7.17.3 name and tag of image ES_HOST kitodo-es host of elastic search ES_REST_PORT 9200 rest port ES_NODE_PORT 9300 node port", "title": "Elastic Search"}, {"location": "setup/configuration/environment-variables/#active-mq", "text": "Name Default Description MQ_IMAGE markusweigelt/activemq:latest name and tag of image MQ_HOST kitodo-mq host of active mq MQ_PORT 61616 port of active mq", "title": "Active MQ"}, {"location": "setup/configuration/makefile-variables/", "text": "Makefile Variables MODE You may want Docker Compose to either - manage the Controller (besides Production and Manager services itself), or - leave the Controller to be run standalone (and merely control Production and Manager here). Managed Mode This is the default setting in the Makefile, and can be made explicit via export MODE=managed Equivalently, when using docker-compose without the Makefile, it is recommended to export all config files into a variable once (so you won't have to type -f docker-compose.kitodo-app.yml each time): export COMPOSE_FILE=docker_compose.yml:docker_compose.managed.yml:docker-compose.kitodo-app.yml Standalone Mode In this mode, you build, configure, start and stop the OCR-D Controller externally (and possibly remotely) . When using the Makefile, just set export MODE=standalone in your shell once. Equivalently, when using docker-compose without the Makefile, you should set export COMPOSE_FILE=docker_compose.yml:docker-compose.kitodo-app.yml In addition, you must also configure where the Manager can find the standalone Controller. For example , you may want to set: export MODE=standalone CONTROLLER_ENV_UID=$(id -u) CONTROLLER_HOST=ocrserver CONTROLLER_PORT_SSH=8022", "title": "Makefile Variables"}, {"location": "setup/configuration/makefile-variables/#makefile-variables", "text": "", "title": "Makefile Variables"}, {"location": "setup/configuration/makefile-variables/#mode", "text": "You may want Docker Compose to either - manage the Controller (besides Production and Manager services itself), or - leave the Controller to be run standalone (and merely control Production and Manager here).", "title": "MODE"}, {"location": "setup/configuration/makefile-variables/#managed-mode", "text": "This is the default setting in the Makefile, and can be made explicit via export MODE=managed Equivalently, when using docker-compose without the Makefile, it is recommended to export all config files into a variable once (so you won't have to type -f docker-compose.kitodo-app.yml each time): export COMPOSE_FILE=docker_compose.yml:docker_compose.managed.yml:docker-compose.kitodo-app.yml", "title": "Managed Mode"}, {"location": "setup/configuration/makefile-variables/#standalone-mode", "text": "In this mode, you build, configure, start and stop the OCR-D Controller externally (and possibly remotely) . When using the Makefile, just set export MODE=standalone in your shell once. Equivalently, when using docker-compose without the Makefile, you should set export COMPOSE_FILE=docker_compose.yml:docker-compose.kitodo-app.yml In addition, you must also configure where the Manager can find the standalone Controller. For example , you may want to set: export MODE=standalone CONTROLLER_ENV_UID=$(id -u) CONTROLLER_HOST=ocrserver CONTROLLER_PORT_SSH=8022", "title": "Standalone Mode"}, {"location": "usage/handling/", "text": "Handling of Docker Compose Starting To start containers from images for all services make start (or equivalently:) docker-compose up -d Stopping To stop containers for all services make stop (or equivalently:) docker-compose stop Stopping and removing To stop containers for all services, and then remove the stopped containers as well as any created networks: make down (or equivalently:) docker-compose down Dumping To see the complete configuration for Docker Compose: make config (or equivalently:) docker-compose config Status To get a list of currently running containers: make status (or equivalently:) docker-compose ps", "title": "Handling of Docker Compose"}, {"location": "usage/handling/#handling-of-docker-compose", "text": "", "title": "Handling of Docker Compose"}, {"location": "usage/handling/#starting", "text": "To start containers from images for all services make start (or equivalently:) docker-compose up -d", "title": "Starting"}, {"location": "usage/handling/#stopping", "text": "To stop containers for all services make stop (or equivalently:) docker-compose stop", "title": "Stopping"}, {"location": "usage/handling/#stopping-and-removing", "text": "To stop containers for all services, and then remove the stopped containers as well as any created networks: make down (or equivalently:) docker-compose down", "title": "Stopping and removing"}, {"location": "usage/handling/#dumping", "text": "To see the complete configuration for Docker Compose: make config (or equivalently:) docker-compose config", "title": "Dumping"}, {"location": "usage/handling/#status", "text": "To get a list of currently running containers: make status (or equivalently:) docker-compose ps", "title": "Status"}, {"location": "usage/kitodo-production/", "text": "Kitodo.Production Open your browser and navigate to http://localhost:8080/kitodo after OCR-D and Kitodo are started. Enter the user name testadmin and the password test in the login dialog. Note: If it is the first launch of Kitodo.Production, then the Indexing tab of the system page is displayed, because indexing still needs to be done. To perform the indexing, click on the button Create ElasticSearch mapping . After the mapping is created, click on the button Start indexing next to the Whole index label. After a few seconds, the index is created and you can navigate to the dashboard by clicking on the Kitodo.Production logo. Execute OCR script step From the dashboard, navigate to All processes by clicking on the button in processes widget, or use the URL http://localhost:8080/kitodo/pages/processes.jsf?tabIndex=0. Select process for OCR, and click on Possible actions and then on Execute KitodoScript . Type following text in script field: action:runscript stepname:OCR script:OCR ... and click on Execute KitodoScript . (This will run the simplistic Tesseract-based default workflow asynchronously. The process status will change as soon as the job is finished.) Watch docker logs , or look under the hood with the Monitor. More configuration options Using project-specific OCR Workflows in Kitodo.Production", "title": "Kitodo.Production"}, {"location": "usage/kitodo-production/#kitodoproduction", "text": "Open your browser and navigate to http://localhost:8080/kitodo after OCR-D and Kitodo are started. Enter the user name testadmin and the password test in the login dialog. Note: If it is the first launch of Kitodo.Production, then the Indexing tab of the system page is displayed, because indexing still needs to be done. To perform the indexing, click on the button Create ElasticSearch mapping . After the mapping is created, click on the button Start indexing next to the Whole index label. After a few seconds, the index is created and you can navigate to the dashboard by clicking on the Kitodo.Production logo.", "title": "Kitodo.Production"}, {"location": "usage/kitodo-production/#execute-ocr-script-step", "text": "From the dashboard, navigate to All processes by clicking on the button in processes widget, or use the URL http://localhost:8080/kitodo/pages/processes.jsf?tabIndex=0. Select process for OCR, and click on Possible actions and then on Execute KitodoScript . Type following text in script field: action:runscript stepname:OCR script:OCR ... and click on Execute KitodoScript . (This will run the simplistic Tesseract-based default workflow asynchronously. The process status will change as soon as the job is finished.) Watch docker logs , or look under the hood with the Monitor.", "title": "Execute OCR script step"}, {"location": "usage/kitodo-production/#more-configuration-options", "text": "Using project-specific OCR Workflows in Kitodo.Production", "title": "More configuration options"}, {"location": "usage/ocrd-monitor/", "text": "OCR-D Monitor Provides a simplistic Web interface under http://localhost:5000 for - browsing workspaces with OCR-D Browser to inspect intermediate/final processing results - getting statistics of running and terminated jobs - reading and searching Docker logs", "title": "OCR-D Monitor"}, {"location": "usage/ocrd-monitor/#ocr-d-monitor", "text": "Provides a simplistic Web interface under http://localhost:5000 for - browsing workspaces with OCR-D Browser to inspect intermediate/final processing results - getting statistics of running and terminated jobs - reading and searching Docker logs", "title": "OCR-D Monitor"}]}